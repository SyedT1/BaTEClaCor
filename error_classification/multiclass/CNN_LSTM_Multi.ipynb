{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e323b2a0-01b5-48f2-acfd-863e4d088e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 14:08:09.270610: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 14:08:09.303805: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 14:08:09.305153: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-02 14:08:09.925267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da13b4e0-4af0-4eb9-ac8a-2a5ece53b2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def replace_strings(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\u00C0-\\u017F\"          #latin\n",
    "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
    "                               \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
    "    \n",
    "    text=emoji_pattern.sub(r'', text)\n",
    "    text=english_pattern.sub(r'', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(my_str):\n",
    "    # define punctuation\n",
    "    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b£|¢|\u0007Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰⚽️✌�￰৷￰'''\n",
    "    \n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "\n",
    "def joining(text):\n",
    "    out=' '.join(text)\n",
    "    return out\n",
    "\n",
    "def preprocessing(text):\n",
    "    out=remove_punctuations(replace_strings(text))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35eba986-011f-4ae1-a544-b30b7ae84f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/train_data.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/test_data.csv'\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_test = pd.read_csv(test_url)\n",
    "stop_words_df = pd.read_excel('stopwords_bangla.xlsx',index_col=False)\n",
    "STOPWORDS = set([word.strip() for word in stop_words_df['words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1b8b22-98af-489a-9ca9-ef32cf534d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['Comment'] = df_train.Comment.apply(lambda x: preprocessing(str(x)))\n",
    "df_test['Comment'] = df_test.Comment.apply(lambda x:preprocessing(str(x)))\n",
    "df = pd.concat([df_train,df_test],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa150e53-3047-4b3b-9fe8-9749b0ab8ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Code Switching', 'Grammatical', 'Multiple Errors', 'Spelling', nan}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['Category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0e424f-adfc-4fb8-8dcb-db48c2fbd565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    d = {\n",
    "        \"Code Switching\":0,\n",
    "        \"Grammatical\":1,\n",
    "        \"Multiple Errors\":2,\n",
    "        \"Spelling\":3\n",
    "    }\n",
    "    if s in d:\n",
    "        return d[s]\n",
    "    else:\n",
    "        return 4\n",
    "df['Category'] = df.Category.apply(lambda x: encode(x))\n",
    "df_train['Category'] = df_train.Category.apply(lambda x: encode(x))\n",
    "df_test['Category'] = df_test.Category.apply(lambda x: encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d9ca621-c173-48fd-8574-5a536f83a0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "402/402 [==============================] - 10s 21ms/step - loss: 1.1052 - accuracy: 0.5929 - val_loss: 0.9815 - val_accuracy: 0.6362\n",
      "Epoch 2/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.9378 - accuracy: 0.6571 - val_loss: 0.9403 - val_accuracy: 0.6486\n",
      "Epoch 3/10\n",
      "402/402 [==============================] - 9s 21ms/step - loss: 0.8055 - accuracy: 0.6943 - val_loss: 0.9762 - val_accuracy: 0.6269\n",
      "Epoch 4/10\n",
      "402/402 [==============================] - 9s 21ms/step - loss: 0.6669 - accuracy: 0.7424 - val_loss: 1.1123 - val_accuracy: 0.6045\n",
      "Epoch 5/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.5375 - accuracy: 0.7914 - val_loss: 1.2753 - val_accuracy: 0.5976\n",
      "Epoch 6/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.4141 - accuracy: 0.8461 - val_loss: 1.5179 - val_accuracy: 0.5553\n",
      "Epoch 7/10\n",
      "402/402 [==============================] - 9s 21ms/step - loss: 0.3218 - accuracy: 0.8808 - val_loss: 1.7341 - val_accuracy: 0.5622\n",
      "Epoch 8/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.2471 - accuracy: 0.9095 - val_loss: 2.0196 - val_accuracy: 0.5311\n",
      "Epoch 9/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.1907 - accuracy: 0.9334 - val_loss: 2.1978 - val_accuracy: 0.5734\n",
      "Epoch 10/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.1402 - accuracy: 0.9477 - val_loss: 2.4227 - val_accuracy: 0.5634\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 2.5394 - accuracy: 0.5540\n",
      "Test Loss: 2.5393834114074707, Test Accuracy: 0.553953230381012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "data = df_train['Comment']\n",
    "labels = df_train['Category']\n",
    "# Tokenize text data\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['Comment'])\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = 100  # Adjust this based on your data\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = tf.keras.utils.to_categorical(labels, num_classes=5)\n",
    "\n",
    "# Create CNN-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_sequence_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=10, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data (replace with your test data)\n",
    "test_data = df_test['Comment'] # Replace with your actual test data\n",
    "test_labels = df_test['Category']  # Replace with your actual test labels\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "Y_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7396b6c-ed7f-4555-8ffd-9e204ebc9bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 8ms/step\n",
      "Test set\n",
      "  Loss: 2.539\n",
      "  Accuracy: 0.554\n",
      "  Macro Recall: 0.400\n",
      "  Macro Precision: 0.412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate macro recall\n",
    "macro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "macro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "# Print the results\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a981e-a317-4f29-aecb-5117b79e7594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
