{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e323b2a0-01b5-48f2-acfd-863e4d088e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 12:27:42.489764: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 12:27:42.521483: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 12:27:42.522952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-02 12:27:43.087127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da13b4e0-4af0-4eb9-ac8a-2a5ece53b2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def replace_strings(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\u00C0-\\u017F\"          #latin\n",
    "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
    "                               \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
    "    \n",
    "    text=emoji_pattern.sub(r'', text)\n",
    "    text=english_pattern.sub(r'', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(my_str):\n",
    "    # define punctuation\n",
    "    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b£|¢|\u0007Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰⚽️✌�￰৷￰'''\n",
    "    \n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "\n",
    "def joining(text):\n",
    "    out=' '.join(text)\n",
    "    return out\n",
    "\n",
    "def preprocessing(text):\n",
    "    out=remove_punctuations(replace_strings(text))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35eba986-011f-4ae1-a544-b30b7ae84f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/train_data.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/test_data.csv'\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_test = pd.read_csv(test_url)\n",
    "stop_words_df = pd.read_excel('stopwords_bangla.xlsx',index_col=False)\n",
    "STOPWORDS = set([word.strip() for word in stop_words_df['words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1b8b22-98af-489a-9ca9-ef32cf534d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['Comment'] = df_train.Comment.apply(lambda x: preprocessing(str(x)))\n",
    "df_test['Comment'] = df_test.Comment.apply(lambda x:preprocessing(str(x)))\n",
    "df = pd.concat([df_train,df_test],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa150e53-3047-4b3b-9fe8-9749b0ab8ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Code Switching', 'Grammatical', 'Multiple Errors', 'Spelling', nan}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['Category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0e424f-adfc-4fb8-8dcb-db48c2fbd565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    d = {\n",
    "        \"Code Switching\":0,\n",
    "        \"Grammatical\":1,\n",
    "        \"Multiple Errors\":2,\n",
    "        \"Spelling\":3\n",
    "    }\n",
    "    if s in d:\n",
    "        return s\n",
    "    else:\n",
    "        return 4\n",
    "df['Category'] = df.Category.apply(lambda x: encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ee2599-1662-4469-bc8a-b7879d14e783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12986 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=50000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=False)\n",
    "tokenizer.fit_on_texts(df['Comment'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e0a637-c409-44f8-8d78-aeeb1c3f0476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Comment'].values)\n",
    "X = pad_sequences(X, maxlen=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491d6ff8-57d6-4328-8b1c-f2f0e38e868c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['Category'],columns=df[\"Category\"]).values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e25308e8-b618-46ad-acd6-245249614564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm = round(len(df)*0.80)\n",
    "X_train, X_test, Y_train, Y_test = X[:lm],X[lm:],Y[:lm],Y[lm:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a1795fe-8c4f-48fd-a0de-f5b6a5429eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8038, 3000) (8038, 5)\n",
      "(2009, 3000) (2009, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d9ca621-c173-48fd-8574-5a536f83a0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(50000,100,input_length=3000))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d6e55d-d9b8-4f6e-8ef9-e0616f919822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "114/114 [==============================] - 230s 2s/step - loss: 1.1641 - accuracy: 0.5687 - val_loss: 1.0963 - val_accuracy: 0.5808\n",
      "Epoch 2/7\n",
      "114/114 [==============================] - 253s 2s/step - loss: 0.9464 - accuracy: 0.6475 - val_loss: 1.0636 - val_accuracy: 0.6306\n",
      "Epoch 3/7\n",
      "114/114 [==============================] - 260s 2s/step - loss: 0.6381 - accuracy: 0.7729 - val_loss: 1.0802 - val_accuracy: 0.6405\n",
      "Epoch 4/7\n",
      "114/114 [==============================] - 290s 3s/step - loss: 0.3729 - accuracy: 0.8728 - val_loss: 1.1387 - val_accuracy: 0.6493\n",
      "Epoch 5/7\n",
      "114/114 [==============================] - 266s 2s/step - loss: 0.2488 - accuracy: 0.9197 - val_loss: 1.3659 - val_accuracy: 0.6443\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=7, batch_size=64,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7396b6c-ed7f-4555-8ffd-9e204ebc9bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 15s 237ms/step - loss: 1.3608 - accuracy: 0.6272\n",
      "63/63 [==============================] - 15s 241ms/step\n",
      "Test set\n",
      "  Loss: 1.361\n",
      "  Accuracy: 0.627\n",
      "  Macro Recall: 0.467\n",
      "  Macro Precision: 0.552\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate macro recall\n",
    "macro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "macro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "# Print the results\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(accr[0], accr[1], macro_recall, macro_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
