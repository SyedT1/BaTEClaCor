{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25f244-e3bd-43c3-be00-08681c7c9103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397c499d-6411-4b85-b366-19c222de90df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 14:53:58.495187: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 14:53:58.526607: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-02 14:53:58.527287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-02 14:53:59.093107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def replace_strings(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\u00C0-\\u017F\"          #latin\n",
    "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
    "                               \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
    "    \n",
    "    text=emoji_pattern.sub(r'', text)\n",
    "    text=english_pattern.sub(r'', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(my_str):\n",
    "    # define punctuation\n",
    "    punctuations = '''````£|¢|Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰⚽️✌�￰৷￰'''\n",
    "    \n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "\n",
    "def joining(text):\n",
    "    out=' '.join(text)\n",
    "    return out\n",
    "\n",
    "def preprocessing(text):\n",
    "    out=remove_punctuations(replace_strings(text))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23085f0-e691-4f77-8c8b-a2677a806414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/train_data.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/FromScratch/error%20detection/data/data/test_data.csv'\n",
    "df_train = pd.read_csv(train_url)\n",
    "df_test = pd.read_csv(test_url)\n",
    "stop_words_df = pd.read_excel('data/stopwords_bangla.xlsx',index_col=False)\n",
    "STOPWORDS = set([word.strip() for word in stop_words_df['words']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632d867a-6e1a-489a-919b-8b4067a18b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['Comment'] = df_train.Comment.apply(lambda x: preprocessing(str(x)))\n",
    "df_test['Comment'] = df_test.Comment.apply(lambda x:preprocessing(str(x)))\n",
    "df = pd.concat([df_train,df_test],ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b796e828-12b1-4347-b428-08893da5b6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    d = {\n",
    "        \"Code Switching\":0,\n",
    "        \"Grammatical\":1,\n",
    "        \"Multiple Errors\":2,\n",
    "        \"Spelling\":3\n",
    "    }\n",
    "    if s in d:\n",
    "        return d[s]\n",
    "    else:\n",
    "        return 4\n",
    "df['Category'] = df.Category.apply(lambda x: encode(x))\n",
    "df_train['Category'] = df_train.Category.apply(lambda x: encode(x))\n",
    "df_test['Category'] = df_test.Category.apply(lambda x: encode(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5734c2d4-cef8-40b5-8f1c-faebef07e3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 14:54:04.538547: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 3600 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/402 [============================>.] - ETA: 0s - loss: 1.1788 - accuracy: 0.5683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 14:54:12.752845: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 3600 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402/402 [==============================] - 10s 21ms/step - loss: 1.1787 - accuracy: 0.5683 - val_loss: 1.1426 - val_accuracy: 0.5883\n",
      "Epoch 2/10\n",
      "402/402 [==============================] - 8s 20ms/step - loss: 1.1220 - accuracy: 0.5846 - val_loss: 1.0070 - val_accuracy: 0.6244\n",
      "Epoch 3/10\n",
      "402/402 [==============================] - 8s 20ms/step - loss: 0.9481 - accuracy: 0.6535 - val_loss: 0.9472 - val_accuracy: 0.6561\n",
      "Epoch 4/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.8674 - accuracy: 0.6819 - val_loss: 0.9513 - val_accuracy: 0.6468\n",
      "Epoch 5/10\n",
      "402/402 [==============================] - 9s 22ms/step - loss: 0.8095 - accuracy: 0.6990 - val_loss: 1.0359 - val_accuracy: 0.5933\n",
      "Epoch 6/10\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.7631 - accuracy: 0.7133 - val_loss: 1.0215 - val_accuracy: 0.6238\n",
      "Epoch 7/10\n",
      "402/402 [==============================] - 8s 20ms/step - loss: 0.7170 - accuracy: 0.7232 - val_loss: 1.0287 - val_accuracy: 0.6381\n",
      "Epoch 8/10\n",
      "402/402 [==============================] - 10s 26ms/step - loss: 0.6793 - accuracy: 0.7396 - val_loss: 1.0978 - val_accuracy: 0.6312\n",
      "Epoch 9/10\n",
      "402/402 [==============================] - 9s 22ms/step - loss: 0.6491 - accuracy: 0.7511 - val_loss: 1.1200 - val_accuracy: 0.6219\n",
      "Epoch 10/10\n",
      "402/402 [==============================] - 9s 21ms/step - loss: 0.6198 - accuracy: 0.7635 - val_loss: 1.1625 - val_accuracy: 0.6095\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.2114 - accuracy: 0.5937\n",
      "Test Loss: 1.211353063583374, Test Accuracy: 0.5937344431877136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Sample data and labels (replace with your own dataset)\n",
    "data = df_train['Comment']\n",
    "labels = df_train['Category']\n",
    "\n",
    "# Tokenize text data\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = 100  # Adjust this based on your data\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = tf.keras.utils.to_categorical(labels, num_classes=5)\n",
    "\n",
    "# Create LSTM with Attention model\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(max_words, 100)(input_layer)\n",
    "lstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n",
    "\n",
    "# Attention mechanism using TensorFlow Addons\n",
    "attention = Attention()([lstm_layer, lstm_layer])\n",
    "attention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
    "\n",
    "output_layer = Dense(5, activation='sigmoid')(attention)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=10, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data (replace with your test data)\n",
    "test_data = df_test['Comment'] # Replace with your actual test data\n",
    "test_labels = df_test['Category']  # Replace with your actual test labels\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "Y_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813bca92-3606-4d8d-b917-7005f531d29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/63 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 14:56:13.033590: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 3600 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 9ms/step\n",
      "Test set\n",
      "  Loss: 1.211\n",
      "  Accuracy: 0.594\n",
      "  Macro Recall: 0.392\n",
      "  Macro Precision: 0.440\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate macro recall\n",
    "macro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "macro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
    "# Print the results\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
